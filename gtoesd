#!/usr/bin/env python

import argparse
import xml.parsers.expat
import socket
import json
import time
import sys
import zlib
import httplib
import logging
import logging.handlers
import random
import os

class gmeta_processor:
    #
    # Processes XML stats from ganglia gmetad and injects them into
    # elasticsearch via the bulk API. Designed to run as a deamon
    # process collecting stats at a regular interval and pushing to
    # ES.
    #
    def __init__ (
            self,
            gmeta_hosts,
            elastic_hosts,
            interval  = 15,
            batch     = 10000,
            debug     = False,
            basename  = 'ganglia',
            utctime   = True,
            slavemode = False,
    ):
        self.debug = debug
        self.gmeta_hosts = gmeta_hosts
        self.elastic_hosts = elastic_hosts
        self.interval = interval
        self.batch = batch
        self.cur_host = None
        self.cur_grid = 'unspecified'
        self.cur_cluster = 'unspecified'
        self.time_reported = 0
        self.readbytes = 8192
        self.basename = basename
        self.index_count = 0
        self.slave = False
        if slavemode:
            self.slave = True
        #
        # The localtime function does not give proper tz info
        #
        self.utctime = utctime
        if utctime:
            self.tz = '+0000'
        else:
            self.tz = time.strftime('%z')
        self.buffer = []
        self.n_parsed_metrics = 0
        self.n_parsed_hosts = 0
        #
        # Start elasticsearch keepalive connection if not debug mode
        #
        if not debug:
            self.elastic_connect()

    def make_index_name (self, reftime = None):
        if reftime:
            ref = reftime
        else:
            if self.utctime:
                ref = time.gmtime()
            else:
                ref = time.localtime()

        return time.strftime(self.basename+'-%Y.%m.%d', ref)

    def s_el (self, name, attrs):
        #
        # XML start element processor. For grid and cluster types
        # store the name. For host type weget the reported type and
        # store host name and number processed. Metric is where the
        # real magic happens of created index entries
        #
        if name == 'GRID':
            self.cur_grid = attrs['NAME']
        if name == 'CLUSTER':
            self.cur_cluster = attrs['NAME']
        if name == 'HOST':
            self.cur_host = attrs['NAME']
            self.time_reported = int(attrs['REPORTED'])
            self.n_parsed_hosts += 1
        if name == 'METRIC' and self.cur_host:
            self.n_parsed_metrics += 1
            tn = int(attrs['TN'])
            time_metric = self.check_time - tn
            #
            # Metrics reported less than interval time ago should be
            # pushed to the system. Maybe we also need to check that
            # host last updated time is within last interval
            # (otherwise we may have already done this one)
            #
            if tn > self.interval:
                return
            if self.utctime:
                metric_reftime = time.gmtime(time_metric)
            else:
                metric_reftime = time.localtime(time_metric)

            time_metric = time.strftime(
                "%Y-%m-%dT%H:%M:%S", metric_reftime
            )
            time_metric += self.tz
            units = attrs['UNITS']
            if units.strip() == "":
                units = "-"
            #
            # The index itself, which gets converted to json
            #
            out = {
                'name': attrs['NAME'],
                'units': units,
                '@timestamp': time_metric,
                'host': self.cur_host,
                'cluster': self.cur_cluster,
                'grid': self.cur_grid,
            }
            val = attrs['VAL']
            if attrs['TYPE'] == 'float' or attrs['TYPE'] == 'double':
                out['val'] = float(val)
            elif attrs['TYPE'] != 'string':
                out['val'] = int(val)
            else:
                out['stringval'] = val
            #
            # Override the index if needed. Dont send each time to
            # conserve on bytes
            #
            indexname = self.make_index_name(metric_reftime)

            if indexname == self.indexname:
                self.buffer.append('{"index":{}}')
            else:
                self.buffer.append(
                    '{"index":{"_index":"%s"}}' %(indexname)
                )
            self.buffer.append(json.dumps(out))

    def e_el (self, name):
        #
        # XML end element parser. All we need to do here is close out
        # host entry so no processing any cruft
        #
        global host
        if name == 'HOST':
            self.cur_host = None

    def elastic_connect (self):
        #
        # Try connecting to one of any specified elasticsearch nodes
        # in order and return 1 if successful or 0 if failed. The
        # connection is stored and used in the elastic_index function.
        #
        for e in self.elastic_hosts:
            addr,port = e.split(':')
            self.elasticconn = httplib.HTTPConnection(addr,int(port))
            try:
                self.elasticconn.connect()
                return 1
            except:
                logging.warning('failed to connect es host %s:%s' %(addr,port))

        logging.error('failed to connect to any es host')
        return 0

    def ganglia_rotate (self):
        #
        # put current server (front of list) at the end. Useful to
        # de-prioritize hosts that fail connection or give bad XML
        #
        self.gmeta_hosts.append(self.gmeta_hosts.pop(0))
        logging.debug('rotated ganglia hosts, next try to %s' %(
            self.gmeta_hosts[0]
        ))

    def elastic_request (self, req, data = None):
        hdr = { "Content-Type":"text/json" }

        meth = 'GET'
        if data:
            meth = 'POST'

        logging.debug ('sending %s request to elastic endpoint %s' %(meth,req))

        try:
            self.elasticconn.request(meth, req, data, hdr)
        except:
            logging.debug ('request to elasticsearch failed')
            if self.elastic_connect():
                try:
                    self.elasticconn.request(meth, req, data, hdr)
                except:
                    logging.debug ('second request to elasticsearch failed')
                    return 0
            else:
                logging.debug ('failed to make elasticsearch connection')
                return 0
        return 1

    def elastic_index (self):
        #
        # Index whatever is in the buffer at present. Attempt a
        # reconnect to elasticsearch and inspect the output for errors
        # if required and configured
        #
        logging.debug('Submitting batch of size %d to index %s' %(
            len(self.buffer)/2, self.indexname,
        ))

        if self.debug:
            logging.debug('indexing disabled')
            return 1
        #
        # Skip if we have nothing to index
        #
        if len(self.buffer) == 0:
            return 1

        bulk = '\n'.join(self.buffer)
        req = '/%s/gmon/_bulk' %(self.indexname)
        #
        # Try sending the request to elasticsearch on current
        # connection. This may fail if elastic goes down or otherwise
        # closes the connection. Retry connection on all supplied
        # servers and reissue request if possible, otherwise we have
        # no choice but to give up
        #
        if not self.elastic_request (req, bulk):
            return 0
        #
        # Attempt to get the response from server, if this fails we
        # are probably in a bad state so clean up connection and, and
        # skip this one (we quite possibly sent already)
        #
        try:
            res = self.elasticconn.getresponse()
        except:
            logging.error('Got exception in es http response:',sys.exc_info())
            self.elasticconn.close()
            self.elastic_connect()
            return 0
        #
        # We have to read the response here for reusing the
        # connection, for non OK statuses as well
        #
        try:
            rdata = json.loads(res.read())
        except:
            return 0

        if res.status == 200:
            if rdata['errors']:
                logging.warning('errors detected in es response')
                #
                # possibly inspect errors in indexing. We should
                # probably only do this if some debug level logging is
                # requested, otherwise just continue
                #
                c = 0
                for i in rdata['items']:
                    if i['create']['status'] != 201:
                        logging.debug('index error source: %s, message: %s' %(
                            self.buffer[c*2+1],i
                        ))
                    c+=1
        else:
            logging.error('Got bad return from es: %d %s' %(
                res.status, res.reason
            ))
            return 0
        return 1

    def parse_metrics (self):
        #
        # Connect to Ganglia and get XML representation of
        # cluster. Feed into the XML parser and occasionally trigger
        # the elastic bulk indexing after batching (batch parameter)
        #
        p=xml.parsers.expat.ParserCreate()
        p.StartElementHandler = self.s_el
        p.EndElementHandler = self.e_el
        xmldata=''
        #
        # connect to ganglia. Try servers listed in turn for a server
        # which we can connect to
        #
        for i in xrange(len(self.gmeta_hosts)):
            s = socket.socket()
            addr,port = self.gmeta_hosts[0].split(':')
            try:
                s.connect((addr,int(port)))
                xmldata = s.recv(self.readbytes)
                break
            except:
                self.ganglia_rotate()
                logging.error('Could not ganglia source %s:%s' %(addr,port))
                pass

        #
        # check if we never connected, then skip the rest of this run
        # if so. If in slave mode, we must stop here because others
        # will detect lack of records and start competing for master
        #
        if xmldata == '':
            self.slave = True
            logging.error('Failed to connect to any ganglia source')
            return

        # clear counters and defaults
        self.buffer = []
        self.n_parsed_metrics = 0
        self.n_parsed_hosts = 0
        self.cur_grid = 'unspecified'
        self.cur_cluster = 'unspecified'
        self.check_time = time.time()
        # set the presumed index name (overidden if needed)
        self.indexname = self.make_index_name()

        # check for zipped data, use zlib autdetection MAX_WBITS|32
        zipped = False
        if xmldata[0:5] != '<?xml':
            zipped = True
            decomp = zlib.decompressobj(zlib.MAX_WBITS|32)
        while xmldata != '':
            try:
                if zipped:
                    p.Parse(decomp.decompress(xmldata))
                else:
                    p.Parse(xmldata)
            except:
                logging.error('Error %d parsing XML from %s: %s at %d' %(
                    p.ErrorCode,
                    self.gmeta_hosts[0],
                    xml.parsers.expat.ErrorString(p.ErrorCode),
                    p.ErrorByteIndex,
                ))
                #
                # If there is corrupt XML, may be some issue with this
                # gmond, try connections from the other host if
                # specified
                #
                logging.debug('rotating ganglia hosts due to XML error')
                self.ganglia_rotate()
                #
                # And we will not be able to process any more from
                # here, so stop with this run.
                #
                break

            if len(self.buffer) >= self.batch:
                if self.elastic_index():
                    self.slave = False
                self.buffer = []

            xmldata = s.recv(self.readbytes)

        s.close()
        if self.elastic_index():
            self.slave = False
        self.buffer = []

    def get_index_count (self):
        #
        # Query elasticsearch stats API for the current doc count of
        # the current index
        #
        indexname = self.make_index_name()

        req = '/%s/_stats/docs' %(indexname)

        logging.debug('sending elastic stats request: %s' %(req))

        if not self.elastic_request (req):
            return -1

        try:
            res = self.elasticconn.getresponse()
            res = json.loads(res.read())
            res = int(res['_all']['primaries']['docs']['count'])
            logging.debug('elastic stats number of docs: %d' %(res))
        except:
            logging.debug('no/bad response from elastic stats request')
            return -1

        return res

    def index_slave_record (self):
        #
        # Insert the slave master vote record into the index, return 1
        # upon success and 0 otherwise
        #
        indexname = self.make_index_name()

        req = '/%s/gtoesd/' %(indexname)
        dat = '{"host":"%s","@timestamp":"%s"}' %(
            os.getenv('HOSTNAME'),
            time.strftime("%Y-%m-%dT%H:%M:%S"),
        )

        self.elastic_request (req, dat)

        try:
            res = self.elasticconn.getresponse()
            res = res.read()
            logging.debug('result of slave record is: %s' %(res))
            if res.status == 201:
                return 1
        except:
            pass
        return 0

    def check_slave_status (self):
        #
        # Check status of metric indexing. If we are in slave mode, we
        # need to check to see if there are more items than last
        # run. If not, we should attempt to become master. Election is
        # held by having all slaves try to index one item using random
        # delays. Contention is checked by ensuring only one more
        # entry than last after a short settling time.
        #
        count = self.get_index_count()

        #
        # If we got bad return from count, we cant do anything
        #
        if count == -1:
            logging.debug ('slave mode: bad stats return')
            return
        #
        # First time? if so skip. Index grew? this means someone else
        # is master so we don't do anything
        #
        if self.index_count == 0 or count - self.index_count > 0:
            logging.debug ('slave mode: other master detected (%d records)' %(
                count - self.index_count
            ))
            self.index_count = count
            return

        #
        # Now we assume there is no-one updating the stats, so we
        # begin the election. Loop using random sleep intervals until
        # someone is selected master
        #
        logging.debug ('slave mode: no master detected, starting election')
        self.index_count = count

        #
        # Each potential slave does a random sleep and then attempts
        # to put a record into the index if another slave did not
        # already do so.
        #
        while True:
            time.sleep(random.uniform(0,10))
            count = self.get_index_count()
            #
            # did someone beat us to it? otherwise try to put record
            #
            if count - self.index_count > 0:
                logging.debug ('slave mode: alternate slave wins vote')
                break

            if not self.index_slave_record():
                logging.debug ('slave mode: error adding our vote record')
                break
            #
            # Finally for paranoia sake if another slave timing is so
            # close to ours, give a little time before checking status
            #
            time.sleep(0.3)
            count = self.get_index_count()
            #
            # Only if ours is the only record to arrive since starting
            # election cycle to we become master
            #
            if count - self.index_count == 1:
                logging.debug ('slave mode: transition out of slave mode')
                self.slave = False
                break

    def run (self):
        while True:
            start_time = time.time()
            if self.slave:
                self.check_slave_status()
            else:
                #
                # Pre-empt slave mode, only when we have successfully
                # indexed to elasticsearch will we come back out of
                # slave mode
                #
                self.slave = True
                self.parse_metrics()
                logging.debug('parsed %d metrics from %d hosts' %(
                    self.n_parsed_metrics, self.n_parsed_hosts,
                ))
            sleep_time = self.interval-(time.time()-start_time)
            if sleep_time < 0:
                sleep_time = 0
            logging.debug('sleeping %f seconds' %(sleep_time))
            time.sleep(sleep_time)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Index ganglia metrics to Elasticsearch')
    parser.add_argument(
        '-g','--ganglia',
        type=str,help='gmetad host:port',
        metavar='H:P',action='append',
    )
    parser.add_argument(
        '-e','--elastic',
        type=str,help='elastic host:port',
        metavar='H:P',action='append',
    )
    parser.add_argument(
        '-i','--interval',
        type=int,help='poll in seconds',
        metavar='I',default=15,
    )
    parser.add_argument(
        '-n','--noindex',
        help='Do not index, just show what would happen',
        action='store_true',
    )
    parser.add_argument(
        '-b','--basename',
        help='base name of index e.g. {basename}-YYYY.mm.dd',
        type=str,default='ganglia',
    )
    parser.add_argument(
        '--localtime',
        help='use localtime instead of UTC for index and events',
        action='store_true',
    )
    parser.add_argument(
        '-v','--verbose',
        help='increase verbosity, -vv for debug logs',
        action='count',default=0,
    )
    parser.add_argument(
        '-c','--console',
        help='log to console instead of syslog',
        action='store_true',
    )
    parser.add_argument(
        '-s','--slave',
        help='run in slave mode for HA clustering',
        action='store_true',
    )

    p = parser.parse_args()

    #
    # setup our logging based on any passed option
    #
    rootlog = logging.getLogger()
    syslog  = logging.handlers.SysLogHandler('/dev/log')
    syslog.setFormatter(logging.Formatter('gtoesd: %(message)s'))
    #
    # Set the verbosity, starting at default of warning, we should
    # subtract 10 to get to next level for each -v option
    #
    rootlog.setLevel(logging.WARNING-10*p.verbose)
    if p.console:
        rootlog.addHandler(logging.StreamHandler())
    else:
        rootlog.addHandler(syslog)
    logging.debug('initialized logging interface')

    if not p.ganglia:
        p.ganglia = ['localhost:8651']
    if not p.elastic:
        p.elastic = ['localhost:9200']

    g = gmeta_processor(
        p.ganglia,
        p.elastic,
        interval=p.interval,
        debug=p.noindex,
        basename=p.basename,
        utctime=not p.localtime,
        slavemode=p.slave,
    )
    g.run()
